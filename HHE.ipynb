{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c1f71c",
   "metadata": {},
   "source": [
    "# Author: \"Matougui Zakaria\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ad922",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, recall_score, accuracy_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import joblib\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from deslib.des import METADES\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import wilcoxon\n",
    "from researchpy import ttest\n",
    "from easy_mpl import taylor_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62261af1",
   "metadata": {},
   "source": [
    "# Input DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full_datafreme is the DataFrame containing all the data that has been pre-processed \n",
    "\n",
    "# Features is the DataFrame containing all the causative factors \n",
    "Features=Full_datafreme.drop(\"Target\", axis=1)\n",
    "\n",
    "# Target is a Series or DataFrame containing the target variable (Landslides)\n",
    "Target=Full_datafreme['Target']\n",
    "\n",
    "# Randomly select examples from the majority class to balance the class distribution\n",
    "rus=RandomUnderSampler(random_state=0)\n",
    "\n",
    "# X and y will contain your balanced dataset, where the number of samples in each class is the same\n",
    "X,y=rus.fit_resample(Features,Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f6382",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcb82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of baselearners\n",
    "def B_models():\n",
    "    models_all = [\n",
    "        ('Logistic Regression', LogisticRegression()),\n",
    "        ('Decision Tree', DecisionTreeClassifier(random_state=rng)),\n",
    "        ('SVM', SVC(probability=True,random_state=rng)),\n",
    "        ('Gaussian Naive Bayes', GaussianNB()),\n",
    "        ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "        ('Linear Discriminant Analysis', LinearDiscriminantAnalysis()),\n",
    "        ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n",
    "        ('MLP Classifier', MLPClassifier(random_state=rng))\n",
    "    ]\n",
    "    return models\n",
    "# Add the prefered models to the liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate and plot VIF\n",
    "def calculate_and_plot_vif(Features):\n",
    "    # Create a DataFrame to store the VIF values\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = Features.columns\n",
    "\n",
    "    # Calculate VIF for each feature\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(Features.values, i)\n",
    "                      for i in range(len(Features.columns))]\n",
    "\n",
    "    # Create a figure and axis for plotting\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Create a bar plot of VIF values\n",
    "    sns.barplot(x='feature', y='VIF', data=vif_data.sort_values(by=['VIF']), ax=ax, palette='Spectral')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    sns.color_palette(\"flare\", as_cmap=True)\n",
    "    ax.set_xlabel('Causative factors')\n",
    "    \n",
    "    # Add labels to the bars\n",
    "    for c in ax.containers:\n",
    "        ax.bar_label(c, fmt='%.2f', label_type='edge', padding=2)\n",
    "    \n",
    "    # Adjust spacing\n",
    "    ax.margins(y=0.1)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(Featurs):\n",
    "   \n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = Featurs.corr()\n",
    "\n",
    "    # Create a figure and axis for the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "\n",
    "    # Set the plot title\n",
    "    plt.title(\"Correlation Matrix Heatmap\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdac5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot roc curves\n",
    "def plot_roc_curve(result_table):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i in result_table.index:\n",
    "        plt.plot(result_table.loc[i]['fpr'], \n",
    "                 result_table.loc[i]['tpr'], \n",
    "                 label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
    "\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "    plt.legend(prop={'size': 13}, loc='lower right')\n",
    "    plt.grid(visible=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e3311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate feature sensitivity for a given model\n",
    "def Sensitivity(model, X, y):\n",
    "    results = []  # Initialize an empty list to store sensitivity results\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate the baseline AUC score on the validation data\n",
    "    baseline_auc = roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "    # Iterate through each predictor (feature) in the dataset\n",
    "    for predictor in X.columns:\n",
    "        X_val_copy = X_val.copy()  # Create a copy of the validation data\n",
    "        X_val_copy[predictor] = np.random.permutation(X_val_copy[predictor])  # Permute the values of the predictor\n",
    "        y_pred = model.predict_proba(X_val_copy)[:, 1]  # Predict probabilities with altered predictor\n",
    "        auc_diff = baseline_auc - roc_auc_score(y_val, y_pred)  # Calculate AUC difference\n",
    "        results.append({'Factor': predictor, 'AUC_Difference': auc_diff})  # Store factor and AUC difference\n",
    "\n",
    "    SA_data = pd.DataFrame(results)  # Create a DataFrame from sensitivity results\n",
    "    return SA_data  # Return the sensitivity data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b136d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot sensitivity analysis results\n",
    "def plot_sensitivity_results(results_df):\n",
    "    # Create a horizontal bar plot to visualize AUC differences for each factor\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(results_df['Factor'], results_df['AUC_Difference'], color='skyblue')\n",
    "    plt.xlabel('AUC Difference')  # Set the label for the x-axis\n",
    "    plt.title('Sensitivity Analysis - AUC Difference for Each Factor')  # Set the plot title\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)  # Add grid lines for reference\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6301224",
   "metadata": {},
   "source": [
    "# Features analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(Featurs)\n",
    "calculate_and_plot_vif(Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1299ab0",
   "metadata": {},
   "source": [
    "# Base learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d02f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed\n",
    "rng = 1\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=rng)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rng)\n",
    "\n",
    "# Define hyperparameters for each model\n",
    "param_grid = {\n",
    "    'Logistic Regression': {'C': [0.01, 0.1, 1, 10], \n",
    "                            'penalty': ['l1', 'l2']},\n",
    "    \n",
    "    'Decision Tree': {'max_depth': [None, 4,5,6,7,8,9,10], \n",
    "                      'min_samples_split': [2, 5, 10]},\n",
    "    \n",
    "    'SVM': {'C': [0.01, 0.1, 1, 10], \n",
    "            'kernel': ['linear', 'rbf'], \n",
    "            'gamma': [0.1, 1, 10,'scale', 'auto']},\n",
    "    \n",
    "    'Gaussian Naive Bayes': {\"var_smoothing\": [1e-8,1e-9,1e-10]},\n",
    "    \n",
    "    'K-Nearest Neighbors': {'n_neighbors': np.arange(2,60,2), \n",
    "                            'p':[1,2,3,4,5],\n",
    "                            'weights': ['uniform', 'distance']},\n",
    "    \n",
    "    'Linear Discriminant Analysis': {},\n",
    "    \n",
    "    'Quadratic Discriminant Analysis': {'tol':[0.0001,0.001]},\n",
    "    \n",
    "    'MLP Classifier': {'hidden_layer_sizes': [np.arange(4,50,2) ], \n",
    "                       'activation': ['relu', 'tanh'],\n",
    "                       'solver':[\"lbfgs\", 'sgd', 'adam'],\n",
    "                      'alpha':[0.001,0.01,0.1]}\n",
    "}\n",
    "\n",
    "B_models_name=['LR','DT','SVM','NB','KNN','LDA','QDA','MLP']\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "best_params = {}\n",
    "models = B_models()\n",
    "\n",
    "fitted_models = {}\n",
    "\n",
    "for name, model in models:\n",
    "    if name in param_grid:\n",
    "        grid = GridSearchCV(estimator=model, param_grid=param_grid[name], scoring='roc_auc', cv=cv)\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_model = grid.best_estimator_\n",
    "        fitted_models[name] = best_model\n",
    "\n",
    "        # Save the best model using joblib\n",
    "        joblib.dump(best_model, f\"{name}_best_model.pkl\")\n",
    "\n",
    "# Print the best parameters\n",
    "for name, params in best_params.items():\n",
    "    print(f\"Best parameters for {name}: {params}\")\n",
    "\n",
    "# Evaluate models and store results in a DataFrame\n",
    "results_df = pd.DataFrame(columns=['Model', 'RMSE', 'AUC', 'RECALL', 'ACCURACY', 'KAPPA'])\n",
    "\n",
    "for name, model in fitted_models.items():\n",
    "    model.fit(X_train, y_train)  \n",
    "    y_pred = model.predict(X_test)  \n",
    "\n",
    "    # Calculate metrics\n",
    "    rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    # Add results to the DataFrame\n",
    "    results_df = results_df.append({'Model': name, 'RMSE': rmse, 'AUC': auc,\n",
    "                                    'RECALL': recall, 'ACCURACY': accuracy,\n",
    "                                    'KAPPA': kappa}, ignore_index=True)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f00b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_table = pd.DataFrame(columns=['Models', 'fpr', 'tpr', 'auc'])\n",
    "\n",
    "for name, model in fitted_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    yproba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, yproba)\n",
    "    auc = roc_auc_score(y_test, yproba)\n",
    "    \n",
    "    result_table = result_table.append({'Models': name,\n",
    "                                        'fpr': fpr, \n",
    "                                        'tpr': tpr, \n",
    "                                        'auc': auc}, ignore_index=True)\n",
    "sns.set_theme()\n",
    "\n",
    "result_table.set_index('Models', inplace=True)\n",
    "\n",
    "plot_roc_curve(result_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc5519",
   "metadata": {},
   "source": [
    "# Heterogeneous ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tuples for the ensembles\n",
    "estimators = [(name, model) for name, model in fitted_models.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c6edb",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060bfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacking= StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(),cv=10)\n",
    "Stacking.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688df718",
   "metadata": {},
   "source": [
    "### METADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ae6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for base learners and meta-learner\n",
    "Xb, X_m, yb, y_m = train_test_split(X_train, y_train, test_size=0.5, random_state=rng)\n",
    "\n",
    "# Extract second elements (classifiers) from the tuples\n",
    "classifiers = [classifier for name, classifier in estimators]\n",
    "\n",
    "# Train base classifiers\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(Xb, yb)\n",
    "\n",
    "# Train the METADES ensemble\n",
    "metades = METADES(pool_classifiers=classifiers,\n",
    "                  meta_classifier=LogisticRegression(),\n",
    "                  k=6, mode='hybrid', random_state=rng, n_jobs=2)\n",
    "metades.fit(X_m, y_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119dfe8",
   "metadata": {},
   "source": [
    "###  Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting=VotingClassifier(estimators=estimators, voting='soft',weights=None)\n",
    "Voting.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4568fd08",
   "metadata": {},
   "source": [
    "### Weighted voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ROC AUC scores for each model\n",
    "roc_auc_scores = {}\n",
    "for name, model in fitted_models.items():\n",
    "    y_probas = model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_probas)\n",
    "    roc_auc_scores[name] = roc_auc\n",
    "\n",
    "# Calculate the weights based on ROC AUC scores\n",
    "total_auc = sum(roc_auc_scores.values())\n",
    "weights = {name: auc / total_auc for name, auc in roc_auc_scores.items()}\n",
    "\n",
    "# Create the weighted voting classifier\n",
    "WE_voting= VotingClassifier(estimators=estimators, voting='soft', weights=list(weights.values()))\n",
    "\n",
    "WE_voting.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of heterogeneous ensembles\n",
    "\n",
    "Het_en=[Stacking,metades,Voting,WE_voting]\n",
    "Het_en_names=['Stacking','METADES','Voting','Weighted']\n",
    "results_df_het = pd.DataFrame(columns=['Model', 'RMSE', 'AUC', 'RECALL', 'ACCURACY', 'KAPPA'])\n",
    "\n",
    "for name, model in zip(Het_en_names,Het_en):\n",
    "    y_pred = model.predict(X_test)  \n",
    "\n",
    "    # Calculate metrics\n",
    "    rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    # Add results to the DataFrame\n",
    "    results_df_het = results_df_het.append({'Model': name, 'RMSE': rmse, 'AUC': auc,\n",
    "                                    'RECALL': recall, 'ACCURACY': accuracy,\n",
    "                                    'KAPPA': kappa}, ignore_index=True)\n",
    "\n",
    "# Print the results\n",
    "print(results_df_het)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_het = pd.DataFrame(columns=['Models', 'fpr', 'tpr', 'auc'])\n",
    "\n",
    "for name, model in zip(Het_en_names,Het_en):\n",
    "    yproba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, yproba)\n",
    "    auc = roc_auc_score(y_test, yproba)\n",
    "    \n",
    "    result_table_het = result_table_het.append({'Models': name,\n",
    "                                        'fpr': fpr, \n",
    "                                        'tpr': tpr, \n",
    "                                        'auc': auc}, ignore_index=True)\n",
    "\n",
    "result_table_het.set_index('Models', inplace=True)\n",
    "#Plot roc curves\n",
    "plot_roc_curve(result_table_het)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653dbcb2",
   "metadata": {},
   "source": [
    "# Homogeneous ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier(random_state=rng)\n",
    "\n",
    "# Define the hyperparameters\n",
    "param_grid_RF = {\n",
    "    'n_estimators': np.arange(50,200,25),\n",
    "    'max_depth': [None, 5,6,7,8],\n",
    "    'min_samples_split': [2, 5,10],\n",
    "    'min_samples_leaf': [1, 2,4,6]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV instance\n",
    "grid_search_RF = GridSearchCV(estimator=RF, param_grid=param_grid_RF, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "\n",
    "RF=grid_search_RF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADA = AdaBoostClassifier(random_state=rng)\n",
    "\n",
    "# Define the hyperparameters\n",
    "param_grid_ADA = {\n",
    "    'n_estimators': np.arange(50,200,25),\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search_ADA = GridSearchCV(estimator=ada_model, param_grid=param_grid_ADA, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search_ADA.fit(X, y)\n",
    "\n",
    "# Get the best estimator\n",
    "ADA=grid_search_ADA.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd540242",
   "metadata": {},
   "outputs": [],
   "source": [
    "BG=BaggingClassifier(base_estimator=fitted_models['Decision Tree'],random_state=rng)\n",
    "\n",
    "# Define hyperparameters\n",
    "param_grid_BG = {\n",
    "    'n_estimators': np.arange(50,200,25),\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for the BaggingClassifier\n",
    "grid_search_BG = GridSearchCV(estimator=BG, param_grid=param_grid_BG, scoring='roc_auc', cv=cv)\n",
    "grid_search_BG.fit(X, y)\n",
    "\n",
    "# Get the best estimator\n",
    "\n",
    "BG = grid_search_BG.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85149743",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS=BaggingClassifier(base_estimator=fitted_models['Decision Tree'],bootstrap=False,random_state=rng)\n",
    "\n",
    "# Define hyperparameters\n",
    "param_grid_RSS = {\n",
    "    'n_estimators': np.arange(50,200,25),\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for the BaggingClassifier\n",
    "grid_search_RSS = GridSearchCV(estimator=BG, param_grid=param_grid_RSS, scoring='roc_auc', cv=cv)\n",
    "grid_search_RSS.fit(X, y)\n",
    "\n",
    "# Get the best estimator\n",
    "\n",
    "RSS = grid_search_BG.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a595b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hom_en=[RSS,ADA,BG,RF]\n",
    "Hom_en_names=['RSS','Adb','BG','RF']\n",
    "\n",
    "results_df_hom = pd.DataFrame(columns=['Model', 'RMSE', 'AUC', 'RECALL', 'ACCURACY', 'KAPPA'])\n",
    "\n",
    "for name, model in zip(Hom_en_names,Hom_en):\n",
    "    y_pred = model.predict(X_test)  \n",
    "\n",
    "    # Calculate metrics\n",
    "    rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    # Add results to the DataFrame\n",
    "    results_df_hom = results_df_hom.append({'Model': name, 'RMSE': rmse, 'AUC': auc,\n",
    "                                    'RECALL': recall, 'ACCURACY': accuracy,\n",
    "                                    'KAPPA': kappa}, ignore_index=True)\n",
    "\n",
    "# Print the results\n",
    "print(results_df_het)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6108619",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_hom = pd.DataFrame(columns=['Models', 'fpr', 'tpr', 'auc'])\n",
    "\n",
    "for name, model in zip(Hom_en_names,Hom_en):\n",
    "    yproba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, yproba)\n",
    "    auc = roc_auc_score(y_test, yproba)\n",
    "    \n",
    "    result_table_hom = result_table_hom.append({'Models': name,\n",
    "                                        'fpr': fpr, \n",
    "                                        'tpr': tpr, \n",
    "                                        'auc': auc}, ignore_index=True)\n",
    "\n",
    "result_table_hom.set_index('Models', inplace=True)\n",
    "\n",
    "#Plot roc curves\n",
    "plot_roc_curve(result_table_hom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_models=models+Het_en+Hom_en\n",
    "All_models_names=B_models_name+Het_en_names+Hom_en_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8ede7",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab730c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data=pd.DataFrame()\n",
    "\n",
    "# Iterate through the list of models and add predictions to the DataFrame\n",
    "for name, model in zip(All_models_names, All_models):\n",
    "    # Predict probabilities \n",
    "    probs = model.predict_proba(predict_features)\n",
    "    preds = probs[:, 1]  # Extract probabilities\n",
    "    \n",
    "    # Add the predictions as a new column in the DataFrame\n",
    "    predict_data[name] = preds\n",
    "\n",
    "# Now, predict_data contains columns with predictions from each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb643e",
   "metadata": {},
   "source": [
    "# Wilcoxon analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the results\n",
    "wilcoxon_results = {}\n",
    "\n",
    "# Iterate through pairs of models\n",
    "for i in range(len(All_models)):\n",
    "    for j in range(i + 1, len(All_models)):\n",
    "        model1_name = All_models_names[i]\n",
    "        model2_name = All_models_names[j]\n",
    "        \n",
    "        # Get the predictions of the two models to be compared\n",
    "        predictions_model1 = predict_data[model1_name]\n",
    "        predictions_model2 = predict_data[model2_name]\n",
    "        \n",
    "        # Perform the Wilcoxon signed-rank test\n",
    "        _, p_value = stats.wilcoxon(predictions_model1, predictions_model2)\n",
    "        \n",
    "        # Store the p-value in the results dictionary\n",
    "        pair_name = f\"{model1_name} vs {model2_name}\"\n",
    "        wilcoxon_results[pair_name] = p_value\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "wilcoxon_results_df = pd.DataFrame(list(wilcoxon_results.items()), columns=['Model Pair', 'p-value'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1cf58b",
   "metadata": {},
   "source": [
    "# Sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of best models named 'best_models'\n",
    "\n",
    "for model in best_models:\n",
    "    # Perform sensitivity analysis and obtain the results DataFrame\n",
    "    SA_results = FSA_pfi(model, X, y)  # Assuming you have defined the FSA_pfi function\n",
    "\n",
    "    # Plot the sensitivity results\n",
    "    plot_sensitivity_results(SA_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f23fc5",
   "metadata": {},
   "source": [
    "# Taylor Diagram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc177669",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in zip(All_models_names, All_models):\n",
    "    # Predict probabilities \n",
    "    probs = model.predict_proba(X_test)\n",
    "    preds = probs[:, 1] \n",
    "    predict_data[name] = preds\n",
    "\n",
    "Taylor_data = {}\n",
    "\n",
    "for i, model in enumerate(All_models_names):\n",
    "    Taylor_data[model] = X_test[:, i]\n",
    "    \n",
    "Coov=np.cov(predict_data)\n",
    "cov = Coov \n",
    "observations =  y_test\n",
    "simulations =  Taylor_data\n",
    "\n",
    "taylor_plot(observations=observations,\n",
    "            simulations=simulations,plot_bias=True,grid_kws={'axis': 'x', 'color': 'black', 'lw': 0.5},\n",
    "                           leg_kws={'facecolor': 'white',\n",
    "                'edgecolor': 'black','bbox_to_anchor':(1.15, 0.8),\n",
    "                'fontsize': 14, 'labelspacing': 1.0},)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
